# ğŸ“„ RAG Backend â€“ PDF-based Question Answering

This is a **backend-only learning project** that implements a **Retrieval-Augmented Generation (RAG)** system.

The backend:
- accepts **PDF uploads**
- extracts and embeds their content
- stores embeddings in a **local vector database**
- answers user questions **grounded in the uploaded PDFs** using an LLM

This project is intentionally kept simple and is meant for **learning, experimentation, and short-term hosting**.

---

## âœ¨ Features (Current Phase)

- Upload text-based PDF files
- Extract and chunk PDF content
- Generate embeddings locally
- Store embeddings in a local vector database
- Retrieve relevant context for a question
- Generate answers using **Groq LLM**
- Simple REST API (no frontend)

> âš ï¸ User authentication, multi-chat support, and deployment controls are planned for later phases.

---

## ğŸ§  Tech Stack

### Backend
- **Python 3**
- **FastAPI** â€“ REST API framework
- **Uvicorn** â€“ ASGI server

### RAG Pipeline
- **PyPDF** â€“ PDF text extraction
- **Sentence-Transformers** â€“ local embeddings (`all-MiniLM-L6-v2`)
- **ChromaDB (local)** â€“ vector database
- **Groq API** â€“ LLM for answering questions

### Utilities
- **python-dotenv** â€“ environment variables
- **requests** â€“ API calls
- **python-multipart** â€“ file uploads

---

## ğŸ“ Project Structure

    rag-chat-backend/
    â”‚
    â”œâ”€â”€ app/
    â”‚ â”œâ”€â”€ main.py # FastAPI app & routes
    â”‚ â””â”€â”€ rag/
    â”‚ â”œâ”€â”€ pdf_loader.py
    â”‚ â”œâ”€â”€ chunker.py
    â”‚ â”œâ”€â”€ embeddings.py
    â”‚ â”œâ”€â”€ vector_store.py
    â”‚ â””â”€â”€ qa.py
    â”‚
    â”œâ”€â”€ data/
    â”‚ â”œâ”€â”€ uploads/ # Uploaded PDFs (ignored by git)
    â”‚ â””â”€â”€ chroma/ # Vector DB data (ignored by git)
    â”‚
    â”œâ”€â”€ .env # API keys (not committed)
    â”œâ”€â”€ .gitignore
    â”œâ”€â”€ requirements.txt
    â””â”€â”€ README.md


> âš ï¸ The `data/` directory is **intentionally ignored** and regenerated at runtime.

---

## ğŸ” Environment Variables

Create a `.env` file in the project root:
GROQ_API_KEY=your_groq_api_key_here

---

## â–¶ï¸ Running Locally

### 1ï¸âƒ£ Create and activate a virtual environment

```bash
python -m venv venv
source venv/bin/activate   # Windows: venv\Scripts\activate
```

### 2ï¸âƒ£ Install dependencies

```bash
pip install -r requirements.txt
```

### 3ï¸âƒ£ Start the server

```bash
uvicorn app.main:app --reload
```
Server will be available at: ```http://127.0.0.1:8000```
Swagger UI: ```http://127.0.0.1:8000/docs```


## ğŸ§ª How to Test

1. Open the **Swagger UI** in your browser

2. Use `POST /upload` to upload a **text-based PDF file**.

3. After a successful upload, use `POST /ask` to ask questions related to the uploaded document.

If the system is working correctly, the answers returned should be grounded in the content of the uploaded PDF.

---

## âš ï¸ Limitations (Intentional)

- No authentication
- Single shared vector store
- Local storage only
- Embeddings are regenerated if local data is deleted
- Not optimized for production use

These limitations are intentional to keep the project focused on **learning the core RAG workflow**.

---

## ğŸš€ Planned Enhancements

- User authentication using JWT
- Multi-chat support with isolated histories
- Per-chat and global document scoping
- Registration limits for controlled access
- Automatic cleanup of old PDFs
- Deployment on free hosting platforms (e.g., Render)

---

## ğŸ“Œ Purpose

This project is built to:
- understand how **Retrieval-Augmented Generation (RAG)** works internally
- learn backend system design for AI-powered applications
- experiment with **LLMs and vector databases**
- serve as a **learning and portfolio project**

---

